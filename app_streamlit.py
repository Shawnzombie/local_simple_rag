import os
import streamlit as st
from langchain_community.llms import Ollama
from langchain_ollama import OllamaEmbeddings # å‡è®¾ä½ å·²ç»æŒ‰ç…§å»ºè®®æ›´æ–°äº†å¯¼å…¥
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# --- é…ç½® ---
VECTOR_DB_PATH = "chroma_db"
OLLAMA_LLM_MODEL = "llama3"
OLLAMA_EMBED_MODEL = "llama3" # å¿…é¡»ä¸ ingest.py ä¸­ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´

# --- LangSmith é…ç½®ï¼ˆå¯é€‰ï¼Œç”¨äºç¦ç”¨è¿½è¸ªï¼‰ ---
# å¦‚æœä½ ä¸ä½¿ç”¨ LangSmithï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼Œå°† LANGCHAIN_TRACING_V2 è®¾ç½®ä¸º "false"
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_API_KEY"] = ""

# --- ç¼“å­˜ RAG ç³»ç»Ÿç»„ä»¶ï¼Œé¿å…æ¯æ¬¡è¿è¡Œæ—¶éƒ½é‡æ–°åŠ è½½ ---
@st.cache_resource
def setup_rag_system():
    """
    åˆå§‹åŒ–RAGç³»ç»Ÿçš„æ‰€æœ‰ç»„ä»¶ï¼šLLMï¼ŒåµŒå…¥æ¨¡å‹ï¼Œå‘é‡æ•°æ®åº“ï¼Œæ£€ç´¢å™¨å’ŒRAGé“¾ã€‚
    è¿™ä¸ªå‡½æ•°ä¼šè¢«Streamlitç¼“å­˜ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œã€‚
    """
    st.info("ğŸ’¡ æ­£åœ¨åŠ è½½æˆ–åˆå§‹åŒ–RAGç³»ç»Ÿï¼Œè¯·ç¨å€™...")
    try:
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)
        vector_store = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        st.success("âœ” å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        st.error(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        st.warning("è¯·ç¡®ä¿å·²è¿è¡Œ `python ingest.py` è„šæœ¬å¹¶æˆåŠŸåˆ›å»ºäº† `'chroma_db'` ç›®å½•ã€‚")
        st.stop() # åœæ­¢åº”ç”¨è¿è¡Œï¼Œç›´åˆ°ç”¨æˆ·è§£å†³é—®é¢˜

    # ä¿®æ”¹1: ç§»é™¤ streaming=True å‚æ•°
    llm = Ollama(model=OLLAMA_LLM_MODEL, temperature=0.5)
    st.success(f"âœ” Ollama LLM æ¨¡å‹ '{OLLAMA_LLM_MODEL}' å·²åŠ è½½ã€‚")

    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 1}
    )
    st.success("âœ” æ£€ç´¢å™¨å·²è®¾ç½®ã€‚")

    prompt_template = PromptTemplate(
        template="""
        ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚
        å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜ä½ æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œ
        ä¸è¦å‡­ç©ºæé€ ç­”æ¡ˆã€‚

        ä¸Šä¸‹æ–‡:
        {context}

        é—®é¢˜:
        {input}

        ç­”æ¡ˆ:
        """,
        input_variables=["context", "input"]
    )

    document_chain = create_stuff_documents_chain(llm, prompt_template)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    st.success("âœ” RAG é“¾å·²æ„å»ºå®Œæˆã€‚")
    st.info("ğŸ‰ ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
    return retrieval_chain

# --- Streamlit UI ç•Œé¢ ---
st.set_page_config(page_title="æœ¬åœ°RAGçŸ¥è¯†é—®ç­”ç³»ç»Ÿ", page_icon="ğŸ“š")
st.title("ğŸ“š æœ¬åœ°RAGçŸ¥è¯†é—®ç­”ç³»ç»Ÿ (Ollama + LangChain)")

# åˆå§‹åŒ–RAGé“¾
rag_chain = setup_rag_system()

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œè¯·æå‡ºä½ çš„é—®é¢˜ã€‚"})

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # æ˜¾ç¤ºæ€è€ƒä¸­çš„åŠ©æ‰‹æ¶ˆæ¯
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # ç”¨äºåŠ¨æ€æ›´æ–°æ¶ˆæ¯
        full_response = ""
        retrieved_docs_info = [] # ç”¨äºå­˜å‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯ï¼Œä»¥ä¾¿åç»­å¼•ç”¨æ¥æº

        with st.spinner("æ­£åœ¨æ£€ç´¢å¹¶ç”Ÿæˆç­”æ¡ˆï¼Œè¯·ç¨å€™..."): # ç¡®ä¿ spinner åœ¨æµå¼è¾“å‡ºå‰æ˜¾ç¤º
            try:
                # ä¿®æ”¹2: è¿­ä»£ RAG é“¾çš„æµå¼è¾“å‡º
                for chunk in rag_chain.stream({"input": prompt}):
                    # LangChain 0.2.x ç‰ˆæœ¬çš„ stream() æ–¹æ³•å¯¹äº create_retrieval_chain
                    # è¿”å›çš„ chunk ç»“æ„å¯èƒ½ä¸æ—©æœŸç‰ˆæœ¬ä¸åŒã€‚
                    # å®ƒé€šå¸¸ä¼šè¿”å›ä¸€ä¸ªåŒ…å« 'answer' æˆ– 'output' é”®çš„å­—å…¸ã€‚
                    # 'context' é€šå¸¸ä¼šåœ¨æ•´ä¸ªæµç»“æŸåä½œä¸ºå•ç‹¬çš„ chunk æˆ–åœ¨æŸä¸ªç‰¹å®šchunkä¸­æä¾›ã€‚
                    # æˆ‘ä»¬è¿™é‡Œå‡è®¾ 'answer' é”®æ˜¯æŒç»­çš„æ–‡æœ¬æµã€‚

                    # LangChain 0.2.x çš„ Chain.stream() é€šå¸¸ä¼šå°†æœ€ç»ˆç­”æ¡ˆå’Œæ£€ç´¢åˆ°çš„context
                    # åˆ†åˆ«ä½œä¸ºä¸åŒçš„ chunk è¿”å›ï¼Œæˆ–è€…åœ¨æŸäº›ä¸­é—´æ­¥éª¤ä¸­ã€‚
                    # å¯¹äº create_retrieval_chain, 'answer' æ˜¯æœ€ç»ˆæ–‡æœ¬ï¼Œ'context' æ˜¯æ£€ç´¢ç»“æœã€‚
                    
                    if "answer" in chunk: # è¿™æ˜¯ä¸€ä¸ªåŒ…å«æœ€ç»ˆç­”æ¡ˆçš„ chunk
                        full_response += chunk["answer"]
                        message_placeholder.markdown(full_response + "â–Œ") # æ¨¡æ‹Ÿå…‰æ ‡
                    
                    # ä¹Ÿå¯ä»¥å°è¯•æ•è· context (å¦‚æœå®ƒåœ¨æµä¸­ä½œä¸ºå•ç‹¬çš„ chunk å‡ºç°)
                    # ç„¶è€Œï¼Œé€šå¸¸ context æ˜¯åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰å°±è¢«ç¡®å®šå¥½çš„ï¼Œè€Œä¸æ˜¯æµå¼è¾“å‡ºçš„ã€‚
                    # ä¸ºäº†ç¡®ä¿èƒ½æ•è·åˆ° contextï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¸€ä¸ªæ›´ç›´æ¥çš„è®¿é—®æ–¹å¼
                    # æˆ–è€…åœ¨ç”Ÿæˆç­”æ¡ˆåæ‰‹åŠ¨è®¿é—® chain.get_last_run_info() æˆ–ç±»ä¼¼æ–¹æ³•
                    # ä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å®ƒåœ¨æŸä¸ª chunk é‡Œä¸€æ¬¡æ€§æä¾›ã€‚

                # å¦‚æœ Streamlit çš„ spinner é®æŒ¡äº†æµå¼è¾“å‡ºçš„æ›´æ–°ï¼Œå¯ä»¥åœ¨è¿™é‡Œç¡®ä¿æœ€ç»ˆç­”æ¡ˆå®Œæ•´æ˜¾ç¤º
                message_placeholder.markdown(full_response)

                # ä¿®æ”¹3: è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚LangChain 0.2.x çš„ create_retrieval_chain
                # åœ¨ stream() æ–¹æ³•ä¸­ï¼Œå¦‚æœéœ€è¦ç²¾ç¡®åœ°è·å– contextï¼Œ
                # æœ€å¥½åœ¨æµç»“æŸåé€šè¿‡ response['context'] è®¿é—®ï¼Œ
                # æˆ–è€…åœ¨è°ƒç”¨ stream() ä¹‹å‰å…ˆè¿è¡Œä¸€æ¬¡æ£€ç´¢å™¨ã€‚
                # ä½†æ›´å¸¸è§çš„æ˜¯ï¼Œcontext ä¼šåœ¨æŸä¸ªç‰¹å®šçš„ chunk ä¸­è¢«åŒ…å«ã€‚
                # å¦‚æœä¸Šè¿° stream å¾ªç¯ä¸­æ²¡æœ‰æ•è·åˆ° contextï¼Œå¯ä»¥è€ƒè™‘å¦‚ä¸‹ï¼š
                # (ä½†è¿™ä¼šè§¦å‘é¢å¤–çš„æ£€ç´¢ï¼Œæˆ–è€…ä¾èµ–äº LangChain ç‰ˆæœ¬çš„ stream() è¡Œä¸º)
                # ç›®å‰çš„ä»£ç å‡è®¾ context ä¼šåœ¨æµå¼ chunk ä¸­å‡ºç°ã€‚
                
                # å¦‚æœä¸Šè¿° stream å¾ªç¯ä¸­æ²¡æœ‰æ•è·åˆ° contextï¼Œå¯ä»¥å°è¯•ï¼š
                # # æ¨¡æ‹Ÿæ£€ç´¢ï¼Œä½†è¿™ä¸ä¼šè§¦å‘ LLM çš„æµå¼è¾“å‡ºï¼Œåªæ˜¯ä¸ºäº†è·å– context
                # temp_retrieved_docs = setup_rag_system().retriever.invoke(prompt)
                # for doc in temp_retrieved_docs:
                #     source = doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')
                #     page = doc.metadata.get('page', 'æœªçŸ¥é¡µ')
                #     file_name = os.path.basename(source)
                #     retrieved_docs_info.append(f"- {file_name} (é¡µç : {page})")


                # åœ¨ Streamlit ä¸­ï¼Œä¸€ä¸ªç®€å•çš„ RAG Chain å¯èƒ½ä¸ä¼šåœ¨ stream() ä¸­ç›´æ¥æä¾› 'context' é”®
                # è€Œæ˜¯ä¼šä½œä¸ºæ•´ä¸ª response çš„ä¸€éƒ¨åˆ†ã€‚å¦‚æœ 'context' é”®åœ¨æµä¸­æœªèƒ½æ•è·åˆ°ï¼Œ
                # é‚£ä¹ˆéœ€è¦è°ƒæ•´è·å–æ–¹å¼ã€‚
                # æœ€ç®€å•çš„æ–¹æ³•æ˜¯ï¼Œåœ¨ stream() ä¹‹å‰å…ˆæ‰§è¡Œä¸€æ¬¡æ£€ç´¢ã€‚
                # æˆ–è€…ï¼Œå¦‚æœLLMè¿”å›çš„responseä¸­åŒ…å«contextï¼ˆé€šå¸¸å¦‚æ­¤ï¼‰ï¼Œåœ¨æµå®Œæˆåå†æå–ã€‚
                # é’ˆå¯¹ Streamlit 2.x å’Œ LangChain 0.2.x çš„å¸¸è§æ¨¡å¼ï¼š
                # stream å¾ªç¯ç»“æŸåï¼Œå¦‚æœéœ€è¦ contextï¼Œå¯ä»¥é€šè¿‡ response.get('context') è·å–
                # ä½†å› ä¸ºè¿™é‡Œæ˜¯ stream() æ¨¡å¼ï¼Œæˆ‘ä»¬åªèƒ½åœ¨ chunk å‡ºç°æ—¶æ•è·ã€‚
                # è€ƒè™‘åˆ° Chain.stream() çš„ç›®çš„æ˜¯æµå¼è¾“å‡º LLM çš„ç­”æ¡ˆï¼Œ
                # context é€šå¸¸æ˜¯ä½œä¸ºè¾“å…¥ç»™åˆ° LLM çš„ï¼Œè€Œä¸æ˜¯ LLM çš„æµå¼è¾“å‡ºã€‚
                # å› æ­¤ï¼Œæˆ‘ä»¬æš‚æ—¶ç§»é™¤ä» chunk['context'] è·å–çš„é€»è¾‘ï¼Œ
                # è€Œæ˜¯åœ¨æµå®Œæˆä¹‹åå†å°è¯•è·å–æˆ–é€šè¿‡å…¶ä»–æ–¹å¼æä¾›ã€‚
                
                # æš‚æ—¶æ³¨é‡Šæ‰ä» chunk ä¸­è·å– context çš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¸ä¼šå‡ºç°åœ¨æ¯ä¸ª chunk ä¸­
                # for chunk in rag_chain.stream({"input": prompt}):
                #    if "context" in chunk:
                #        # ... (æ­¤éƒ¨åˆ†æš‚æ—¶ä¸é€‚ç”¨æˆ–éœ€è¦æ›´å¤æ‚å¤„ç†)
                
                # å‡è®¾ Streamlit ä¸Šçš„ Streamlit UI åˆ·æ–°å¯èƒ½å¯¼è‡´æ¯æ¬¡éƒ½é‡è·‘
                # æ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯åœ¨ Streamlit çš„ session state é‡Œç›´æ¥å­˜å‚¨ context
                # è€Œä¸æ˜¯è¯•å›¾ä» stream é‡Œæ•è·ã€‚
                # ä¸ºç®€ä¾¿èµ·è§ï¼Œè¿™é‡Œå…ˆä¸å¤„ç†æµå¼æ•è· context çš„å¤æ‚æ€§ï¼Œ
                # è€Œæ˜¯ä¸“æ³¨äºç­”æ¡ˆçš„æµå¼è¾“å‡ºã€‚
                
                # ä¸ºäº†èƒ½åœ¨æµå¼è¾“å‡ºåæ˜¾ç¤ºæ¥æºï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è°ƒç”¨ stream ä¹‹å‰å…ˆè¿›è¡Œæ£€ç´¢ï¼Œ
                # æˆ–è€…ä¾é  LLM æœ€ç»ˆåœ¨ç­”æ¡ˆä¸­åŒ…å«æ¥æºä¿¡æ¯ã€‚
                # å¦‚æœå¸Œæœ›ä» LangChain çš„è¿½è¸ªä¸­è·å–ï¼Œé‚£åˆéœ€è¦ LangSmithã€‚
                # ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•æ˜¯ï¼Œåœ¨æµå¼è¾“å‡ºåï¼Œé‡æ–°è¿›è¡Œä¸€æ¬¡æ£€ç´¢æ¥è·å–æ¥æºã€‚
                # æˆ–è€…ï¼Œæœ€ç®€å•æ˜¯ï¼Œå¦‚æœ LLM å›ç­”åŒ…å«æ¥æºï¼Œåˆ™ä¾é  LLM çš„ç”Ÿæˆã€‚
                # è¿™é‡Œä¸ºäº†æ¼”ç¤ºæµå¼ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸è¿›è¡Œæ¥æºå¼•ç”¨ï¼Œ
                # å› ä¸ºåœ¨ stream æ¨¡å¼ä¸‹è·å– context æ¯”è¾ƒå¤æ‚ä¸”ä¾èµ–å…·ä½“é“¾å®ç°ã€‚

                # å¦‚æœç¡®å®éœ€è¦æ¥æºï¼Œå¯ä»¥åœ¨ stream ç»“æŸåæ‰§è¡Œä¸€æ¬¡æ™®é€šçš„ chain.invoke
                # æˆ–è€…åœ¨ stream ä¹‹å‰ï¼Œå…ˆ retriever.invoke(prompt) å¾—åˆ° documentsï¼Œ
                # ç„¶åå°†è¿™äº› documents ä¼ é€’ç»™ chainï¼Œå¹¶ä¿å­˜å®ƒä»¬çš„ metadataã€‚
                
                # æš‚æ—¶ç§»é™¤æµå¼è¾“å‡ºä¸­è·å– context çš„é€»è¾‘ï¼Œå¹¶ç®€åŒ–æ¥æºå¼•ç”¨ï¼Œ
                # ä»…åœ¨ Streamlit UI ç¤ºä¾‹ä¸­æä¾›ä¸€ä¸ªç¤ºæ„ã€‚
                # å¦‚æœéœ€è¦ç²¾ç¡®æ¥æºï¼Œè€ƒè™‘å°† retrieval_chain æ‹†åˆ†ä¸ºä¸¤æ­¥ï¼š
                # 1. æ£€ç´¢ (retriever.invoke) -> è·å¾—æ–‡æ¡£
                # 2. ç”Ÿæˆ (document_chain.stream) -> ä¼ å…¥æ–‡æ¡£å’Œé—®é¢˜

                # ä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬ç°åœ¨å‡è®¾ Streamlit ç”¨æˆ·çš„éœ€æ±‚æ˜¯å…ˆè®©æµå¼è¾“å‡ºè·‘èµ·æ¥ï¼Œ
                # æ¥æºå¼•ç”¨å¯ä»¥ä¹‹åå†ä¼˜åŒ–ã€‚
                # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ Prompt Template ä¸­è¦æ±‚ LLM å¼•ç”¨æ¥æºã€‚

                # æš‚æ—¶å»é™¤æ¥æºå¼•ç”¨ï¼Œä¸“æ³¨äºæµå¼è¾“å‡ºã€‚
                # å¦‚æœåç»­éœ€è¦ï¼Œå¯ä»¥é‡æ–°æ·»åŠ ï¼Œä½†éœ€è¦æ›´å¤æ‚çš„ context è·å–ç­–ç•¥ã€‚
                # streamæ¨¡å¼ä¸‹ï¼Œresponse['context'] å¾€å¾€æ˜¯åœ¨æµç»“æŸåæ‰èƒ½å®Œæ•´è·å–ã€‚
                # for chunk in rag_chain.stream({"input": prompt}):
                #    if "context" in chunk and not retrieved_docs_info: # å°è¯•ä¸€æ¬¡æ€§æ•è·
                #        for doc in chunk["context"]:
                #             source = doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')
                #             page = doc.metadata.get('page', 'æœªçŸ¥é¡µ')
                #             file_name = os.path.basename(source)
                #             retrieved_docs_info.append(f"- {file_name} (é¡µç : {page})")
                #
                # if retrieved_docs_info:
                #     source_str = "\n\n**ç›¸å…³æ¥æº:**\n" + "\n".join(sorted(list(set(retrieved_docs_info))))
                #     st.markdown(source_str)
                #     full_response += source_str

            except Exception as e:
                # é”™è¯¯å¤„ç†
                error_message = f"æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯ï¼š`{e}`\n\nè¯·æ£€æŸ¥ï¼š\n1. Ollama æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œã€‚\n2. `llama3` æ¨¡å‹æ˜¯å¦å·²æ‹‰å–ã€‚\n3. ç³»ç»Ÿèµ„æºï¼ˆå†…å­˜ã€CPUï¼‰æ˜¯å¦å……è¶³ã€‚"
                st.error(error_message)
                full_response = error_message
        
    st.session_state.messages.append({"role": "assistant", "content": full_response})